{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5c6acd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "25423801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5082ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7927ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations = {\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"plz\": \"please\",\n",
    "    \"pls\": \"please\",\n",
    "    \"whr\": \"where\",\n",
    "    \"whrs\": \"where\",\n",
    "    \"af\": \"as f***\",\n",
    "    \"ordr\": \"order\",\n",
    "    \"msg\": \"message\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"b4\": \"before\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"bcoz\": \"because\",\n",
    "    \"bc\": \"because\",\n",
    "    \"atm\": \"at the moment\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"yw\": \"you are welcome\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"luv\": \"love\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"hbd\": \"happy birthday\",\n",
    "    \"prod\": \"product\",\n",
    "    \"qty\": \"quantity\",\n",
    "    \"desc\": \"description\",\n",
    "    \"del\": \"delivery\",\n",
    "    \"exp\": \"experience\",\n",
    "    \"cust\": \"customer\",\n",
    "    \"svc\": \"service\",\n",
    "    \"def\": \"defective\",\n",
    "    \"recvd\": \"received\",\n",
    "    \"pkg\": \"package\",\n",
    "    \"amt\": \"amount\",\n",
    "    \"trans\": \"transaction\",\n",
    "    \"avail\": \"available\",\n",
    "    \"unavail\": \"unavailable\",\n",
    "    \"wrt\": \"with respect to\",\n",
    "    \"diff\": \"different\",\n",
    "    \"prob\": \"problem\",\n",
    "    \"res\": \"resolution\",\n",
    "    \"rep\": \"replacement\",\n",
    "    \"ret\": \"return\",\n",
    "    \"ref\": \"refund\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9a0f4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_keywords = {\n",
    "    \"😡\": \"angry\",\n",
    "    \"😠\": \"angry\",\n",
    "    \"🤬\": \"angry\",\n",
    "    \"👿\": \"angry\",\n",
    "    \"😤\": \"angry\",\n",
    "    \"😒\": \"annoyed\",\n",
    "    \"😞\": \"disappointed\",\n",
    "    \"😔\": \"depressed\",\n",
    "    \"😢\": \"depressed\",\n",
    "    \"😭\": \"depressed\",\n",
    "    \"😩\": \"depressed\",\n",
    "    \"😫\": \"depressed\",\n",
    "    \"😟\": \"depressed\",\n",
    "    \"😕\": \"confused\",\n",
    "    \"😐\": \"neutral\",\n",
    "    \"😑\": \"neutral\",\n",
    "    \"🙂\": \"happy\",\n",
    "    \"😊\": \"happy\",\n",
    "    \"😁\": \"happy\",\n",
    "    \"😃\": \"happy\",\n",
    "    \"😄\": \"happy\",\n",
    "    \"😍\": \"love\",\n",
    "    \"🥰\": \"love\",\n",
    "    \"😘\": \"love\",\n",
    "    \"👍\": \"positive\",\n",
    "    \"🙏\": \"positive\",\n",
    "    \"😇\": \"positive\",\n",
    "    \"😎\": \"positive\",\n",
    "    \"😅\": \"relief\",\n",
    "    \"😬\": \"awkward\",\n",
    "    \"😳\": \"embarrassed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b023066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = \" \".join(text.split())\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'\\.\\.\\.|[^\\w\\s\\U0001F600-\\U0001F64F]', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [abbreviations.get(word, word) for word in words]\n",
    "    corrected_words = [spell.correction(word) for word in words]\n",
    "    filtered_words = [word for word in corrected_words if word not in stop_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    text = \" \".join(lemmatized_words)\n",
    "    for emoji_char, emotion in emotion_keywords.items():\n",
    "        text = text.replace(emoji_char, emotion)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5ba7760d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey please tell wheres order'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Hey, can u plz tell me where’s my order??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f7170476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'receive parcel yet'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"i didn't receive my parcel yet!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "944b4583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'order'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(\"Whr’s my ordr 😡😡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "91fb04c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'endswith'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelivery late af...i want refund now\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[166], line 10\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m corrected_words \u001b[38;5;241m=\u001b[39m [spell\u001b[38;5;241m.\u001b[39mcorrection(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      9\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m corrected_words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m---> 10\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_words]\n\u001b[0;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_words)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emoji_char, emotion \u001b[38;5;129;01min\u001b[39;00m emotion_keywords\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[166], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      8\u001b[0m corrected_words \u001b[38;5;241m=\u001b[39m [spell\u001b[38;5;241m.\u001b[39mcorrection(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m      9\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m corrected_words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m---> 10\u001b[0m lemmatized_words \u001b[38;5;241m=\u001b[39m [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_words]\n\u001b[0;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_words)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emoji_char, emotion \u001b[38;5;129;01min\u001b[39;00m emotion_keywords\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\stem\\wordnet.py:85\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\stem\\wordnet.py:41\u001b[0m, in \u001b[0;36mWordNetLemmatizer._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m['us', 'u']\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_exceptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2106\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   2103\u001b[0m     forms \u001b[38;5;241m=\u001b[39m exceptions[form]\n\u001b[0;32m   2104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;66;03m# 1. Apply rules once to the input to get y1, y2, y3, etc.\u001b[39;00m\n\u001b[1;32m-> 2106\u001b[0m     forms \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mform\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;66;03m# 2. Return all that are in the database (and check the original too)\u001b[39;00m\n\u001b[0;32m   2109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filter_forms([form] \u001b[38;5;241m+\u001b[39m forms)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2083\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy.<locals>.apply_rules\u001b[1;34m(forms)\u001b[0m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n\u001b[1;32m-> 2083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   2084\u001b[0m         form[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(old)] \u001b[38;5;241m+\u001b[39m new\n\u001b[0;32m   2085\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m form \u001b[38;5;129;01min\u001b[39;00m forms\n\u001b[0;32m   2086\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m old, new \u001b[38;5;129;01min\u001b[39;00m substitutions\n\u001b[0;32m   2087\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m form\u001b[38;5;241m.\u001b[39mendswith(old)\n\u001b[0;32m   2088\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2087\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rules\u001b[39m(forms):\n\u001b[0;32m   2083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   2084\u001b[0m         form[: \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(old)] \u001b[38;5;241m+\u001b[39m new\n\u001b[0;32m   2085\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m form \u001b[38;5;129;01min\u001b[39;00m forms\n\u001b[0;32m   2086\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m old, new \u001b[38;5;129;01min\u001b[39;00m substitutions\n\u001b[1;32m-> 2087\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m(old)\n\u001b[0;32m   2088\u001b[0m     ]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'endswith'"
     ]
    }
   ],
   "source": [
    "preprocess_text(\"delivery late af...i want refund now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87a23c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
